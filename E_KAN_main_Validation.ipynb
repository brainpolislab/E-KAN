{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de0cae7",
   "metadata": {},
   "source": [
    "# Import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b06e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "#from sklearn_genetic import GASearchCV\n",
    "from sklearn_genetic import GAFeatureSelectionCV\n",
    "#from sklearn_genetic.space import Categorical, Integer, Continuous\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.cross_decomposition import PLSCanonical\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "#from tpot import TPOTClassifier\n",
    "import shap\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import tensorflow as tf\n",
    "#from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "#import keras \n",
    "#from keras.losses import BinaryCrossentropy\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import subprocess # TO RUN VBEOSA\n",
    "\n",
    "#from scikeras.wrappers import KerasClassifier # TO USE VOTING\n",
    "\n",
    "from EKAN_functions import *\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tabtransformertf.models.fttransformer import FTTransformerEncoder, FTTransformer\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f858da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## DEBUGGER ## TEST WETHER THE CV INDEXES ARE THE SAME\n",
    "#cv1 = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1)\n",
    "#cv2 = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1)\n",
    "#train2_tot=[]\n",
    "#for train2,test2 in cv2.split(feats,target):\n",
    "#    train2_tot.append(train2)\n",
    "\n",
    "#i=0\n",
    "#for train1,test1 in cv1.split(feats,target):\n",
    "#    for index in range(0,len(train1)):\n",
    "#        if train1[index] != train2_tot[i][index]:\n",
    "#            print('PROBLEM')\n",
    "#    i+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d992f8e4",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the real outer loop for shap\n",
    "\n",
    "X_gs = pd.read_csv('your_corrected_folds_x.csv')\n",
    "X_gs = X_gs.drop(columns=['Unnamed: 0'],axis=1)\n",
    "#features=X_gs.columns[0:9]\n",
    "\n",
    "\n",
    "#X_gs = X_gs.drop(columns=features,axis=1)\n",
    "\n",
    "\n",
    "y_gs = pd.read_csv('your_corrected_folds_y.csv')\n",
    "y_gs = y_gs.drop(columns=['Unnamed: 0'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"your_folds_pointers.pickle\", \"rb\") as f:\n",
    "    cv_gs_tot = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869012a",
   "metadata": {},
   "source": [
    "### NESTED CV LOOP (corrected in the loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81fe65c",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e483655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_CV_KN(m,k1,k2,N):\n",
    "    accuracy_means = []\n",
    "    accuracy_vars = []\n",
    "    auc_means = []\n",
    "    auc_vars = []\n",
    "    f1_means = []\n",
    "    f1_vars= []\n",
    "    Runtime_count={}\n",
    "    \n",
    "    # Get all the real outer loop for shap\n",
    "\n",
    "    # Get all the real outer loop for shap\n",
    "\n",
    "    X_gs = pd.read_csv('your_corrected_folds_x.csv')\n",
    "    X_gs = X_gs.drop(columns=['Unnamed: 0'],axis=1)\n",
    "    #features=X_gs.columns[0:9]\n",
    "\n",
    "\n",
    "    #X_gs = X_gs.drop(columns=features,axis=1)\n",
    "\n",
    "\n",
    "    y_gs = pd.read_csv('your_corrected_folds_y.csv')\n",
    "    y_gs = y_gs.drop(columns=['Unnamed: 0'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    with open(\"your_folds_pointers.pickle\", \"rb\") as f:\n",
    "        cv_gs_tot = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    for k in [400]:\n",
    "        outer_results_acc = []\n",
    "        outer_results_auc = []\n",
    "        outer_results_f1 = [] \n",
    "        fold_number=0\n",
    "        Runtime_count[k]=0 \n",
    "        while fold_number<10:\n",
    "            scores_inner=[]\n",
    "            # ESTRACT CORRECTED OUTER FOLD\n",
    "\n",
    "            X_tr,X_v=X_gs.iloc[cv_gs_tot[fold_number][0]],X_gs.iloc[cv_gs_tot[fold_number][1]]\n",
    "            y_tr,y_v=y_gs.iloc[cv_gs_tot[fold_number][0]],y_gs.iloc[cv_gs_tot[fold_number][1]]\n",
    "\n",
    "            # TRY AND CATCH RUNTIMES ERROR OF KANS\n",
    "            try:\n",
    "                models,model_p,dataset,dataset_p=KAN_tr_m(X_tr,y_tr.astype('int64'),X_v,y_v.astype('int64'),k,random_s=True,m=m,fil=True,subs=False,k1=k1,k2=k2,N=N)\n",
    "                # PREDICT\n",
    "                if m=='KAN' or m=='KAN_1':\n",
    "                    y_pred=torch.argmax(model_p(dataset_p['test_input']),dim=1)#model.predict(X_v.values) #ensemble_predictions(model,X_v)\n",
    "\n",
    "                if m=='MLP' or m=='SVM':\n",
    "                    y_pred=model_p.predict(dataset_p)\n",
    "\n",
    "\n",
    "                # ESTIMATING OUTER FOLD METRICS\n",
    "                acc = accuracy_score(y_v, y_pred)\n",
    "                #bal_acc=balanced_accuracy_score(y_v, y_pred)\n",
    "                f1=f1_score(y_v,y_pred)\n",
    "                auc=roc_auc_score(y_v,y_pred)\n",
    "                # store the result\n",
    "                outer_results_acc.append(acc)\n",
    "                #outer_results_bal_acc.append(bal_acc)\n",
    "                outer_results_f1.append(f1)\n",
    "                outer_results_auc.append(auc)\n",
    "                # report progress\n",
    "                print('>acc=%.3f' % (acc))\n",
    "                #print('>bal_accacc=%.3f' % (bal_acc))\n",
    "                print('>F1=%.3f' % (f1))\n",
    "                print('>auc=%.3f' % (auc))\n",
    "                Runtime_count[k]=0       \n",
    "            except RuntimeError:\n",
    "                    Runtime_count[k]=Runtime_count[k]+1\n",
    "                    #fold_number=fold_number-1\n",
    "\n",
    "\n",
    "            fold_number+=1\n",
    "        # summarize the estimated performance of the model\n",
    "        # Assuming outer_results_acc, outer_results_bal_acc, and outer_results_f1 are already defined\n",
    "        accuracy_means.append(np.mean(outer_results_acc))\n",
    "        accuracy_vars.append(np.std(outer_results_acc))\n",
    "\n",
    "        # Append the mean and standard deviation of balanced accuracy results\n",
    "        auc_means.append(np.mean(outer_results_auc))\n",
    "        auc_vars.append(np.std(outer_results_auc))\n",
    "\n",
    "        # Append the mean and standard deviation of F1 score results\n",
    "        f1_means.append(np.mean(outer_results_f1))\n",
    "        f1_vars.append(np.std(outer_results_f1))\n",
    "        print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results_acc), np.std(outer_results_acc)))\n",
    "        print('Accuracy results refer to mean and variance')\n",
    "\n",
    "        print('AUC: %.3f (%.3f)' % (np.mean(outer_results_auc), np.std(outer_results_auc)))\n",
    "        print('AUC results refer to mean and variance')\n",
    "\n",
    "        print('F1: %.3f (%.3f)' % (np.mean(outer_results_f1), np.std(outer_results_f1)))\n",
    "        print('F1 results refer to mean and variance')\n",
    "\n",
    "\n",
    "    return [[accuracy_means,accuracy_vars],[auc_means,auc_vars],[f1_means,f1_vars]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87854132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36987692",
   "metadata": {},
   "source": [
    "# K1 vs N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your arrays\n",
    "k1=[4,10,20,50,70,100]\n",
    "k2=[4,7,10,15,17,20]\n",
    "N=[2,4,6,8,15,20]\n",
    "# Generate all possible pairs\n",
    "pairs = list(itertools.product(k1, N))\n",
    "\n",
    "# Initialize the dictionary to store pair values\n",
    "pair_values_2 = {}\n",
    "\n",
    "# Assign a random value to each pair\n",
    "for pair in pairs:\n",
    "    [[accuracy_means,accuracy_vars],[auc_means,auc_vars],[f1_means,f1_vars]]=my_CV_KN('KAN',k1=pair[0],k2=4,N=pair[1])\n",
    "    pair_values_2[pair] = f1_means\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ecdb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the heatmap matrix\n",
    "heatmap_matrix = np.zeros((len(k1), len(N)))\n",
    "\n",
    "# Fill the heatmap matrix with the values from pair_values\n",
    "for i, x in enumerate(k1):\n",
    "    for j, y in enumerate(N):\n",
    "        heatmap_matrix[i, j] = pair_values_2.get((x, y), 0)[0]\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(heatmap_matrix, annot=True, xticklabels=N, yticklabels=k1, cmap=\"Greens\")\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('k1')\n",
    "plt.title('Heatmap of k1 vs N, k2=20')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13821883",
   "metadata": {},
   "source": [
    "# K1 vs K2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your arrays\n",
    "k1=[4,10,20,50,70,100]\n",
    "k2=[4,7,10,15,17,20]\n",
    "N=[2,4,6,8,15,20]\n",
    "# Generate all possible pairs\n",
    "pairs = list(itertools.product(k1, k2))\n",
    "\n",
    "# Initialize the dictionary to store pair values\n",
    "pair_values_1 = {}\n",
    "\n",
    "# Assign a random value to each pair\n",
    "for pair in pairs:\n",
    "    [[accuracy_means,accuracy_vars],[auc_means,auc_vars],[f1_means,f1_vars]]=my_CV_KN('KAN',k1=pair[0],k2=pair[1],N=20)\n",
    "    pair_values_1[pair] = f1_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b0c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the heatmap matrix\n",
    "heatmap_matrix = np.zeros((len(k1), len(k2)))\n",
    "\n",
    "# Fill the heatmap matrix with the values from pair_values\n",
    "for i, x in enumerate(k1):\n",
    "    for j, y in enumerate(k2):\n",
    "        heatmap_matrix[i, j] = pair_values_1.get((x, y), 0)[0]\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(heatmap_matrix, annot=True, xticklabels=k2, yticklabels=k1,cmap=\"YlGnBu\")\n",
    "plt.xlabel('k2')\n",
    "plt.ylabel('k1')\n",
    "plt.title('Heatmap of k1 vs k2, N=20')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfda9d4",
   "metadata": {},
   "source": [
    "# k2 vs N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac9887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Define your arrays\n",
    "k1=[4,10,20,50,70,100]\n",
    "k2=[4,7,10,15,17,20]\n",
    "N=[2,4,6,8,15,20]\n",
    "\n",
    "# Generate all possible pairs\n",
    "pairs = list(itertools.product(k2, N))\n",
    "\n",
    "# Initialize the dictionary to store pair values\n",
    "pair_values = {}\n",
    "\n",
    "# Assign a random value to each pair\n",
    "for pair in pairs:\n",
    "    [[accuracy_means,accuracy_vars],[auc_means,auc_vars],[f1_means,f1_vars]]=my_CV_KN('KAN',k1=4,k2=pair[0],N=pair[1])\n",
    "    pair_values[pair] = f1_means\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c8ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the heatmap matrix\n",
    "heatmap_matrix = np.zeros((len(k2), len(N)))\n",
    "\n",
    "# Fill the heatmap matrix with the values from pair_values\n",
    "for i, x in enumerate(k2):\n",
    "    for j, y in enumerate(N):\n",
    "        heatmap_matrix[i, j] = pair_values.get((x, y), 0)[0]\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(heatmap_matrix, annot=True, xticklabels=N, yticklabels=k2, cmap='Reds')\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('k2')\n",
    "plt.title('Heatmap of k2 vs N,k1=4')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa98fbb0",
   "metadata": {},
   "source": [
    "# COMPARISONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5874e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA,PCA,FastICA\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import random\n",
    "\n",
    "outer_results_acc = []\n",
    "outer_results_auc = []\n",
    "outer_results_f1 = [] \n",
    "\n",
    "\n",
    "# Get all the real outer loop for shap\n",
    "\n",
    "X_gs = pd.read_csv('your_corrected_folds_x.csv')\n",
    "X_gs = X_gs.drop(columns=['Unnamed: 0'],axis=1)\n",
    "#features=X_gs.columns[0:9]\n",
    "\n",
    "\n",
    "#X_gs = X_gs.drop(columns=features,axis=1)\n",
    "\n",
    "\n",
    "y_gs = pd.read_csv('your_corrected_folds_y.csv')\n",
    "y_gs = y_gs.drop(columns=['Unnamed: 0'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"your_folds_pointers.pickle\", \"rb\") as f:\n",
    "    cv_gs_tot = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "fold_number=0\n",
    "while fold_number<10: # run across all the raw train and test splits\n",
    "\n",
    "    scores_inner=[]\n",
    "    # ESTRACT CORRECTED OUTER FOLD\n",
    "\n",
    "    X_tr,X_v=X_gs.iloc[cv_gs_tot[fold_number][0]],X_gs.iloc[cv_gs_tot[fold_number][1]]\n",
    "    y_tr,y_v=y_gs.iloc[cv_gs_tot[fold_number][0]],y_gs.iloc[cv_gs_tot[fold_number][1]]\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    # UNCOMMENT MODEL TO GET COMPARISONS\n",
    "\n",
    "    ## UNCOMMENT TO VALIDATE SKLEARN MODELS AND XGB\n",
    "\n",
    "    #models,model_p,dataset,dataset_p=KAN_tr_m(X_tr,y_tr.astype('int64'),X_v,y_v.astype('int64')) # int64 for type long\n",
    "\n",
    "\n",
    "    #model=RandomForestClassifier(n_estimators=20)#\n",
    "    #model=AdaBoostClassifier()#XGBClassifier()#svm.SVC()\n",
    "    #model=model.fit(X_train_gs_inner,y_train_gs_inner)\n",
    "    #model=model.fit(X_tr,y_tr)\n",
    "\n",
    "    ## UNCOMMENT TO VALIDATE TABNET\n",
    "\n",
    "    #X_train=X_tr.values\n",
    "    #y_train=y_tr.values[:,0]\n",
    "    #X_test=X_v.values\n",
    "    #y_test=y_v.values[:,0]\n",
    "\n",
    "    # define the model\n",
    "    #model= TabNetClassifier(n_steps=10,optimizer_fn=torch.optim.Adam, lambda_sparse=1e-4, momentum=0.3,\n",
    "                            #scheduler_params={\"step_size\":10, \n",
    "                                            #\"gamma\":0.9},\n",
    "                            #scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                            #)\n",
    "\n",
    "\n",
    "\n",
    "    # fit the model \n",
    "    #model.fit(\n",
    "        #X_train,y_train,\n",
    "        #eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        #eval_name=['train', 'test'],\n",
    "        #eval_metric=['accuracy','balanced_accuracy'],\n",
    "        #max_epochs=200, patience=300,\n",
    "        #batch_size=52, virtual_batch_size=52,\n",
    "        #weights=1,\n",
    "        #drop_last=False,\n",
    "    #) \n",
    "\n",
    "    \n",
    "    ## UNCOMMENT TO VALIDATE KAN MODELS AND E-KAN\n",
    "\n",
    "    ### E-KAN\n",
    "    models,model_p,dataset,dataset_p=KAN_tr_m(X_tr,y_tr.astype('int64'),X_v,y_v.astype('int64'),k=400,random_s=True,m='KAN',fil=True,subs=False,k1=4,k2=4,N=8)\n",
    "\n",
    "    ### BASE AND DEEP KAN\n",
    "    #dataset= {}\n",
    "    #dataset['train_input'] = torch.from_numpy(X_tr.values)\n",
    "    #dataset['test_input'] = torch.from_numpy(X_v.values)\n",
    "    #dataset['train_label'] = torch.from_numpy(y_tr.values[:,0])\n",
    "    #dataset['test_label'] = torch.from_numpy(y_v.values[:,0])\n",
    "\n",
    "    #### DEEP\n",
    "\n",
    "    #model_p = KAN(width=[dataset_p['train_input'].shape[1],8,4,2], grid=5, k=4)\n",
    "\n",
    "    #def train_acc():\n",
    "    #    return torch.mean((torch.argmax(model_p(dataset_p['train_input']), dim=1) == dataset_p['train_label']).float())\n",
    "\n",
    "    #def test_acc():\n",
    "    #    return torch.mean((torch.argmax(model_p(dataset_p['test_input']), dim=1) == dataset_p['test_label']).float())\n",
    "\n",
    "    #results = model.train(dataset, opt=\"LBFGS\", steps=50, metrics=(train_acc, test_acc), loss_fn=torch.nn.CrossEntropyLoss(),lamb=00.9),\n",
    "    #results = model_p.train(dataset_p, opt=\"LBFGS\", steps=20, metrics=(train_acc, test_acc), loss_fn=torch.nn.CrossEntropyLoss(),lamb=0.009)\n",
    "    #results['train_acc'][-1], results['test_acc'][-1]\n",
    "\n",
    "    #### BASE\n",
    "\n",
    "    #model_p = KAN(width=[dataset_p['train_input'].shape[1],1,2], grid=5, k=4)\n",
    "\n",
    "    #def train_acc():\n",
    "    #    return torch.mean((torch.argmax(model_p(dataset_p['train_input']), dim=1) == dataset_p['train_label']).float())\n",
    "\n",
    "    #def test_acc():\n",
    "    #    return torch.mean((torch.argmax(model_p(dataset_p['test_input']), dim=1) == dataset_p['test_label']).float())\n",
    "\n",
    "    #results = model.train(dataset, opt=\"LBFGS\", steps=50, metrics=(train_acc, test_acc), loss_fn=torch.nn.CrossEntropyLoss(),lamb=00.9),\n",
    "    #results = model_p.train(dataset_p, opt=\"LBFGS\", steps=20, metrics=(train_acc, test_acc), loss_fn=torch.nn.CrossEntropyLoss())\n",
    "    #results['train_acc'][-1], results['test_acc'][-1]\n",
    "\n",
    "\n",
    "    # PREDICT\n",
    "\n",
    "    # OTHERS\n",
    "    #y_pred= model.predict(X_v.values) \n",
    "\n",
    "    # KAN\n",
    "    y_pred=torch.argmax(model_p(dataset_p['test_input']), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    # ESTIMATING OUTER FOLD METRICS\n",
    "    acc = accuracy_score(y_v, y_pred)\n",
    "    #bal_acc=balanced_accuracy_score(y_v, y_pred)\n",
    "    f1=f1_score(y_v,y_pred)\n",
    "    auc=roc_auc_score(y_v,y_pred)\n",
    "    # store the result\n",
    "    outer_results_acc.append(acc)\n",
    "    #outer_results_bal_acc.append(bal_acc)\n",
    "    outer_results_f1.append(f1)\n",
    "    outer_results_auc.append(auc)\n",
    "    # report progress\n",
    "    print('>acc=%.3f' % (acc))\n",
    "    #print('>bal_accacc=%.3f' % (bal_acc))\n",
    "    print('>F1=%.3f' % (f1))\n",
    "    print('>auc=%.3f' % (auc))\n",
    "\n",
    "    fold_number+=1\n",
    "\n",
    "\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(outer_results_acc), np.std(outer_results_acc)))\n",
    "print('Accuracy results refer to mean and variance')\n",
    "\n",
    "print('AUC: %.3f (%.3f)' % (np.mean(outer_results_auc), np.std(outer_results_auc)))\n",
    "print('AUC results refer to mean and variance')\n",
    "\n",
    "print('F1: %.3f (%.3f)' % (np.mean(outer_results_f1), np.std(outer_results_f1)))\n",
    "print('F1 results refer to mean and variance')\n",
    "\n",
    "#models=list(scores[0].get('estimator'))\n",
    "''' \n",
    "\n",
    "#DEBUGGER TEST WETHER THE GS MODIFY THE CV WHEN FIT IS APPLIED \n",
    "for i in range(len(controllo)):\n",
    "    for j in range(len(controllo[i][0])):\n",
    "        if controllo[i][0][j] != prova[i][0][j]:\n",
    "            break\n",
    "\n",
    "if i == len(controllo)-1:\n",
    "    if j == len(controllo[i][0])-1:\n",
    "        print(\"The arrays are equal\")\n",
    "else:\n",
    "    print(\"The arrays are not equal\")\n",
    "    print(controllo[i][0][j])\n",
    "    print(prova[i][0][j])\n",
    "\n",
    " ''' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7482b47a",
   "metadata": {},
   "source": [
    "# T-TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354473a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind_from_stats\n",
    "\n",
    "def perform_t_test_with_stats(mean1, std1, n1, mean2, std2, n2, equal_var=True):\n",
    "    \"\"\"\n",
    "    Perform an independent two-sample t-test using sample statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    - mean1: Mean of the first sample\n",
    "    - std1: Standard deviation of the first sample\n",
    "    - n1: Size of the first sample\n",
    "    - mean2: Mean of the second sample\n",
    "    - std2: Standard deviation of the second sample\n",
    "    - n2: Size of the second sample\n",
    "    - equal_var: Boolean indicating whether to assume equal variances (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    - t_stat: The t-test statistic\n",
    "    - p_value: The p-value of the test\n",
    "    \"\"\"\n",
    "    # Perform the t-test using the sample statistics\n",
    "    t_stat, p_value = ttest_ind_from_stats(mean1, std1, n1, mean2, std2, n2, equal_var=equal_var)\n",
    "    \n",
    "    return t_stat, p_value\n",
    "\n",
    "# Example usage\n",
    "mean1 = 0.662  # Mean of the first sample\n",
    "std1 = 0.061   # Standard deviation of the first sample\n",
    "n1 = 10      # Size of the first sample\n",
    "\n",
    "mean2 = 0.542  # Mean of the second sample\n",
    "std2 = 0.085   # Standard deviation of the second sample\n",
    "n2 = 10      # Size of the second sample\n",
    "\n",
    "# Perform the t-test\n",
    "t_stat, p_value = perform_t_test_with_stats(mean1, std1, n1, mean2, std2, n2)\n",
    "print(f\"T-test statistic: {t_stat}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the two means.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the two means.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
