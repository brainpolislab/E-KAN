{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa11670a",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcb729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from EKAN_functions import *\n",
    "\n",
    "\n",
    "# SETTING UP PANDAS VISUALIZATION\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#LOADING DATASET\n",
    "\n",
    "data = pd.read_csv('your_data.csv')\n",
    "data=data.drop(columns=['Unnamed: 0'],axis=1)\n",
    "#data=data.drop(columns=['Unnamed: 0.1'],axis=1)\n",
    "#data=data.drop(columns=['center'],axis=1)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0debd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DUMMY FOR DX CLASSES\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "bin=LabelBinarizer()\n",
    "Dx_cols=bin.fit_transform(data['Dx'])\n",
    "\n",
    "Dx_cols=pd.DataFrame(Dx_cols)\n",
    "Dx_cols.columns=['HC','CHR','ROP','ROD']\n",
    "Dx_cols.head()\n",
    "\n",
    "# ADD ROP AND ROP TO DATA TO PERFORM THE CORRECTION \n",
    "data=pd.concat([data,Dx_cols[['ROP','ROD']]],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d3009",
   "metadata": {},
   "source": [
    "## DATASET PREPARATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bcbb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to maintain data integrity and make it balanced we choose to perform a preliminary classification on \n",
    "# HC vs ROP+ROD and a second one with a different model on ROP vs ROD thiss will prevent the negative effec\n",
    "# of unbalanced dataset on model training, that we can see on the confusion matrix (multiclass script).\n",
    "# therefore the following steps are necessary\n",
    "\n",
    "\n",
    "\n",
    "## DATASET PREPARATION FOR THE FIRST MODEL\n",
    "# Dropping CHR patients\n",
    "\n",
    "data = data.drop(data[data['Dx'] == 1].index) \n",
    "# Binarize the labels (healthy vs non healthy)\n",
    "data['Dx'] = np.where(data['Dx'] > 0, 1, 0)\n",
    "\n",
    "\n",
    "## DATASET PREPARATION FOR THE SECOND MODEL\n",
    "'''\n",
    "# REMOVE HC AND CHR\n",
    "data = data.drop(data[data['Dx'] == 0].index) \n",
    "data = data.drop(data[data['Dx'] == 1].index) \n",
    "\n",
    "# Binarize the labels (ROP vs ROD)\n",
    "data['Dx'] = np.where(data['Dx'] > 2, 1, 0)\n",
    "\n",
    "\n",
    "#data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6613ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68165098",
   "metadata": {},
   "source": [
    "# DEFINITION OF THE PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b4068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import the BioCovariatesRegression class from module\n",
    "path = r'the path to your folder'\n",
    "import sys \n",
    "sys.path.insert(0,path)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Use confounder_correction_classes_1 to model the Dx effect on the correction\n",
    "# Use confounder_correction_classes if you don't want to model the Dx effet\n",
    "from new_confounder_correction_classes_1 import BiocovariatesRegression , StandardScalerDict , ComBatHarmonization\n",
    "\n",
    "# INITIALIZATION\n",
    "\n",
    "# choose the number of features you want to correct\n",
    "vol_feat=np.arange(0,393) # 9 substances + confounders features and 73 sMRI features + 319 fMRI + Dx\n",
    "binary_feat=np.arange(6,9) # smoker, non smoker, occasional smoker\n",
    "\n",
    "# Features that need to be corrected in the LR model with covariates \n",
    "#feat_of_interest={'volumes': {'id': vol_feat, 'categorical': ['SEX','Dx'], 'continuous': ['AGE']}}\n",
    "feat_of_interest={'volumes': {'id': vol_feat, 'categorical': ['SEX','ROP','ROD'], 'continuous': ['AGE']}} #take into account ROP and ROD\n",
    "\n",
    "## INITIALIZE CORRECTION FUNCTIONS ##\n",
    "\n",
    "combat_harm=ComBatHarmonization(cv_method=None,ref_batch=None,regression_fit=1,feat_detail=feat_of_interest,feat_of_no_interest=None)\n",
    "biocov_regre=BiocovariatesRegression(cv_method=None,feat_detail=feat_of_interest,ref_batch=None,output_dict=False)#,binary_feat=binary_feat)\n",
    "scaler_data_fun=StandardScalerDict(std_data=True,std_cov=False,output_dict=True,ref_batch=None)\n",
    "\n",
    "# CREATE PROCESSING PIPELINE #\n",
    "\n",
    "steps_biocov=[('Apply combat Harmonization',combat_harm),('Standardize only data', scaler_data_fun),('Confounder Regression', biocov_regre),('Standardize', StandardScaler())]\n",
    "pipeline_biocov_reg= Pipeline(steps_biocov)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc9d85",
   "metadata": {},
   "source": [
    "# DEFINITION OF CV DATASET BUILDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60da05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = data['Dx']#.astype(int)\n",
    "X = data.drop(['Dx'], axis = 1)\n",
    "\n",
    "#SPLIT DATA INTO TRAIN AND TEST SET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  #X_scaled\n",
    "                                                    test_size =0.30, #by default is 75%-25%\n",
    "                                                    #shuffle is set True by default,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state = 123) #fix random seed for replicability\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "n_splits=5\n",
    "n_repeats=2\n",
    "number_of_folds=n_splits*n_repeats\n",
    "cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=2)#random_state=1) set 2 for ROP vs ROD\n",
    "\n",
    "# CREATION OF CV_DATASET FOR THE OUTER LOOP (TRAIN TEST)\n",
    "#[X_train_gs,y_train_gs,cv_gs]=CV_dataset_builder(X_train,y_train,cv,pipeline_biocov_reg)\n",
    "\n",
    "# CREATION OF CV_DATASET FOR THE OUTER LOOP (THE ONE WE USED)\n",
    "[X_train_gs,y_train_gs,cv_gs]=CV_dataset_builder(X,y,cv,pipeline_biocov_reg)\n",
    "\n",
    "X_train_gs.to_csv('your_folds_corrected_x.csv')\n",
    "y_train_gs.to_csv('your_folds_corrected_y.csv')\n",
    "\n",
    "\n",
    "# Save the array to a file\n",
    "with open(\"your_folds_pointers\", \"wb\") as f:\n",
    "    pickle.dump(cv_gs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd673254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT ALL THE DATASET TO TRAIN THE FINAL MODEL\n",
    "\n",
    "\n",
    "binary_var=['smoker','occasional_smoker','non_smoker','DRINKER'] # for mediator is correct because binary variables are not used\n",
    "confounder_var=['SEX','AGE','ROP','ROD','batch']\n",
    "## ALLOCATING DATA ON DIFFERENT VARIABLES AND PREPARING FOR THE FIT ##\n",
    "# Copy the database on other independent variables\n",
    "X_train_v=X.copy()\n",
    "#X_train_v=pd.concat([X_train_v,y_train], axis=1)\n",
    "\n",
    "\n",
    "#X_test_v=X_test.copy()\n",
    "#X_test_v=pd.concat([X_test_v,y_test], axis=1)\n",
    "\n",
    "# SPLIT BINARY AND CONTINUOUS VARIABLES\n",
    "\n",
    "X_b_train_v=X_train_v[binary_var+confounder_var].copy()\n",
    "#X_b_test_v=X_test_v[binary_var+confounder_var].copy()\n",
    "X_train_v=X_train_v.drop(binary_var,axis=1)\n",
    "#X_test_v=X_test_v.drop(binary_var,axis=1)\n",
    "  \n",
    "# Save confounders in other db & Remove them from the main data, to avoid overflow error\n",
    "    \n",
    "# Save variables\n",
    "X_c_train_v=X_train_v[confounder_var].copy()\n",
    "#X_c_test_v=X_test_v[confounder_var].copy()\n",
    "\n",
    "# drop them from the main dataset\n",
    "X_train_v=X_train_v.drop(confounder_var,axis=1).copy()\n",
    "#X_test_v=X_test_v.drop(confounder_var,axis=1).copy()\n",
    "\n",
    "##### CONTINUOUS #####------------------------------------------------------\n",
    "\n",
    "# Build the dictionary structure to ease data correction\n",
    "X_train_v_dict={'data': X_train_v, 'covariates': X_c_train_v}\n",
    "#X_test_v_dict={'data': X_test_v, 'covariates': X_c_test_v}\n",
    "\n",
    "\n",
    "## PERFORMING THE CORRECTION ##\n",
    "\n",
    "# Train the models that apply the correction on data\n",
    "pipeline_biocov_reg.fit(X_train_v_dict) # Fit pipeline on the training set\n",
    "\n",
    "# Apply the correction estimated previously\n",
    "X_train_v_corr=pipeline_biocov_reg.transform(X_train_v_dict) # Apply fitted pipeline on train\n",
    "#X_test_v_corr=pipeline_biocov_reg.transform(X_test_v_dict) # Apply fitted pipeline on test\n",
    "\n",
    "\n",
    "## SAVE CORRECTED DATA ##\n",
    "\n",
    "# Overwrite the corrected data on the uncorrected one\n",
    "for i in range(0,len(X_train_v_corr)):\n",
    "    X_train_v.iloc[i]=X_train_v_corr[i].copy() #apply correction on test sample to the train set\n",
    "\n",
    "\n",
    "#for i in range(0,len(X_test_v_corr)):\n",
    "    #X_test_v.iloc[i]=X_test_v_corr[i].copy() #apply correction on test sample to the test set\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "##### BINARY #####----------------------------------------------------\n",
    "covars=['Dx','SEX','AGE','batch']\n",
    "binary_var=['smoker','occasional_smoker','non_smoker','DRINKER']\n",
    "'''\n",
    "for feat in binary_var:\n",
    "       \n",
    "    #--This part is common for both train and test beacuse is the training of the regression--\n",
    "\n",
    "    est = sm.Logit(X_b_train_v[feat],X_b_train_v[confounder_var]) \n",
    "    estimates=est.fit()\n",
    "    estimates.params[2]=0 # remove effect of Dx\n",
    "\n",
    "\n",
    "    # save raw smoker var\n",
    "    if feat == 'smoker':\n",
    "        y_s_train=X_b_train_v[feat].copy()\n",
    "        y_s_test=X_b_test_v[feat].copy()\n",
    "\n",
    "    #---\n",
    "    for i in range(0,len(X_b_train_v[feat])):\n",
    "        # Substitue the logit value to the old one for train\n",
    "        X_b_train_v[feat].iloc[i]=max(0,X_b_train_v[feat].iloc[i]-estimates.predict(X_b_train_v[confounder_var].iloc[i])[0])\n",
    "        \n",
    "    for i in range(0,len(X_b_test_v[feat])):\n",
    "        # do the same for test\n",
    "        X_b_test_v[feat].iloc[i]=max(0,X_b_test_v[feat].iloc[i]-estimates.predict(X_b_test_v[confounder_var].iloc[i])[0])\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_b_train_v=X_b_train_v.drop(confounder_var,axis=1).copy()\n",
    "#X_b_test_v=X_b_test_v.drop(confounder_var,axis=1).copy()\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "##### CONCAT BINARY AND CONTINUOUS #####-------------------------------------\n",
    "\n",
    "X_train_v=pd.concat([X_b_train_v,X_train_v],axis=1)\n",
    "#X_test_v=pd.concat([X_b_test_v,X_test_v],axis=1)\n",
    "\n",
    "\n",
    "#SAVE DATA FOR TRAIN\n",
    "y_train=y\n",
    "\n",
    "X_train_v.to_csv('your_dataset_corrected_X.csv')\n",
    "y_train.to_csv('your_dataset_corrected_y.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
